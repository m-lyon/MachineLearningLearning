{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Classification\n",
    "\n",
    "This notebook will go through a machine learning implementation of Multi-class Classification using multivariate logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "As per usual, the first thing we need to do is load some data, and visualise it.\n",
    "This dataset is in a native matlab format however, so we have to use a module from *scipy* to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "%matplotlib inline\n",
    "\n",
    "data = loadmat('data/ex3data1.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is a series of handwritten digits, an example of some of the digits can be seen below. We are going to train a logistic regression in a 'one vs. all' approach in order to classify the images.\n",
    "\n",
    "There are 5000 training examples, where each training example is a 20 x 20 array of grayscale pixels. We can then *unroll* the 2D array of pixels into a 400-dimensional vector, thereby making our $X$ matrix have a size (5000, 400). Additionally we have a 5000-dimensional vector $Y$ representing the labels of each training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'y': array([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ...,\n",
       "        [ 9],\n",
       "        [ 9],\n",
       "        [ 9]], dtype=uint8)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Now we need to train 10 different logistic regression models, luckily we have previously covered the mathematical definition of logistic regression, as well as the vectorized regularized implentation in python. Below is the implementation of the cost function and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "   return (1 / (1 + np.exp(-z)))\n",
    "\n",
    "def cost(theta, X, Y, lmbda):\n",
    "    \n",
    "    # Convert X and Y into matrix data type\n",
    "    if type(X) == np.ndarray:\n",
    "        X = np.matrix(X)\n",
    "    if type(Y) == np.ndarray:\n",
    "        Y = np.matrix(Y)\n",
    "    # Convert theta into matrix data type\n",
    "    if type(theta) == np.ndarray:\n",
    "        theta = np.matrix(theta)\n",
    "        \n",
    "    # Get hypothesis\n",
    "    h = sigmoid(X*theta.T)\n",
    "    m,n = X.shape\n",
    "    # Get regularized term\n",
    "    reg = (lmbda / (2*m)) * (np.sum(np.square(theta[:,1:n])))\n",
    "    # Calculate cost\n",
    "    J = (1/m)*((-Y.T * np.log(h)) - ((1-Y).T * np.log(1-h))) + reg\n",
    "    \n",
    "    return J.item()\n",
    "    \n",
    "def gradient(theta, X, Y, lmbda):\n",
    "    # Convert theta from numpy.ndarray to matrix\n",
    "    theta = np.matrix(theta)\n",
    "    # Get hypothesis\n",
    "    h = sigmoid(X*theta.T)\n",
    "    m,n = X.shape\n",
    "    # Get regularized term\n",
    "    reg = (lmbda / m) * (theta.T)\n",
    "    # Calculate gradient\n",
    "    grad = (1/m)* X.T*(h-Y) + reg\n",
    "    # Remove regularization from theta 0\n",
    "    grad[0,0] = grad[0,0] - reg[0,0]\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a logistic regression model only has two possible outputs, in order to deal with a higher number of classes we need to use *one vs. all*, that is training $K$ different logistic regression models, each capapable of identifying one of the $K$ classes.\n",
    "\n",
    "To do this we will combine all of the $\\theta$ matrices for the $K$ classifiers into one matrix $\\Theta$ where each row represents the weights of a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating theta for label: 1\n",
      "Calculating theta for label: 2\n",
      "Calculating theta for label: 3\n",
      "Calculating theta for label: 4\n",
      "Calculating theta for label: 5\n",
      "Calculating theta for label: 6\n",
      "Calculating theta for label: 7\n",
      "Calculating theta for label: 8\n",
      "Calculating theta for label: 9\n",
      "Calculating theta for label: 10\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def one_vs_all(X, Y, num_labels, lmbda):\n",
    "    # First let's insert a column of 0s into X\n",
    "    X_h = np.insert(X, 0, values=np.ones(len(X)), axis=1)\n",
    "    m,n = X_h.shape\n",
    "    \n",
    "    # Initialize Theta matrix\n",
    "    all_theta = np.zeros((num_labels, n))\n",
    "    # matlab arrays start at 1, therefore we must account for this\n",
    "    for i in range(1, num_labels+1):\n",
    "        # Initialize theta\n",
    "        theta = np.zeros(n)\n",
    "        # Get labels for ith class\n",
    "        y = np.array([1 if label == i else 0 for label in Y])\n",
    "        # Reshape array to vector format\n",
    "        y = np.reshape(y, (m,1))\n",
    "        # Find minimum theta\n",
    "        print('Calculating theta for label: {}'.format(i))\n",
    "        fmin = minimize(fun=cost, x0=theta, args=(X_h, y, lmbda), method='TNC', jac=gradient)\n",
    "        # Add this to Theta matrix\n",
    "        all_theta[i-1,:] = fmin.x\n",
    "    return all_theta\n",
    "\n",
    "Theta = one_vs_all(data['X'], data['y'], 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.38318374e+00  0.00000000e+00  0.00000000e+00 ...  1.30442679e-03\n",
      "  -7.56104463e-10  0.00000000e+00]\n",
      " [-3.18410035e+00  0.00000000e+00  0.00000000e+00 ...  4.46067536e-03\n",
      "  -5.08564614e-04  0.00000000e+00]\n",
      " [-4.79696826e+00  0.00000000e+00  0.00000000e+00 ... -2.86879026e-05\n",
      "  -2.47223922e-07  0.00000000e+00]\n",
      " ...\n",
      " [-7.98492546e+00  0.00000000e+00  0.00000000e+00 ... -8.93653391e-05\n",
      "   7.21303710e-06  0.00000000e+00]\n",
      " [-4.57046976e+00  0.00000000e+00  0.00000000e+00 ... -1.33690730e-03\n",
      "   9.99668724e-05  0.00000000e+00]\n",
      " [-5.40237477e+00  0.00000000e+00  0.00000000e+00 ... -1.16778986e-04\n",
      "   7.99100475e-06  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use $\\Theta$ to predict the digits in our dataset, and so how accurate our model is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all(X, Theta):\n",
    "    # Insert 0s column in X\n",
    "    X_h = np.insert(X, 0, values=np.ones(len(X)), axis=1)\n",
    "    m,n = X_h.shape\n",
    "    # Get number of labels from theta\n",
    "    num_labels = Theta.shape[0]\n",
    "\n",
    "    # convert to matrices\n",
    "    X_h = np.matrix(X_h)\n",
    "    Theta = np.matrix(Theta)\n",
    "    \n",
    "    # compute the probability for each class on each training instance\n",
    "    h = sigmoid(X_h*Theta.T)\n",
    "\n",
    "    # create array of the index with the maximum probability\n",
    "    h_argmax = np.argmax(h, axis=1)\n",
    "    \n",
    "    # because our array was zero-indexed we need to add one for the true label prediction\n",
    "    h_argmax += 1\n",
    "    \n",
    "    return h_argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 94.46%\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_all(data['X'], Theta)\n",
    "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, data['y'])]\n",
    "acc = float(sum(correct)) / float(len(correct))\n",
    "print('accuracy = {0:.2f}%'.format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "almost 95% classification, not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
